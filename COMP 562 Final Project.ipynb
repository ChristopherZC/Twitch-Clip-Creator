{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "## loads text file into python\n",
    "#base_path = \"C:/Users/ChrisC\" ## this should be set to where-ever the text files are kept. Make sure to use forward slashes\n",
    "#base_path = \"/Users/ami_zou/Desktop/COMP562_Project\"\n",
    "base_path = \"\"\n",
    "filename = \"V.STORM VS EG.txt\" ## instead of back slashes\n",
    "\n",
    "path_to_file = os.path.join(base_path, filename)\n",
    "fd = open(path_to_file, 'r', encoding =\"utf8\")\n",
    "\n",
    "## loads the text file into a double array with each message being an index in the first array and each word in the message\n",
    "## as an index in the second array\n",
    "lines = fd.read().split('\\n')\n",
    "\n",
    "## counter for how many messages are in the chatlog\n",
    "m = 0\n",
    "\n",
    "for k in range(len(lines)):\n",
    "    lines[k] = lines[k].split(' ')\n",
    "    m = m + 1\n",
    "    \n",
    "## wordl will be a copy of lines without timestamps or usernames. words will be a list of the words that appear in the chatlog\n",
    "wordl = lines.copy()\n",
    "words = []\n",
    "    \n",
    "## removes the timestamp and the user name, which are stored in the first two elements of each sub-array\n",
    "for k in range(len(wordl)):\n",
    "    wordl[k] = wordl[k][2:]\n",
    "\n",
    "## adds each word to the words array. A word will only be added once, regardless of how many times it appears\n",
    "for k in range(len(wordl)):\n",
    "    for w in wordl[k]:\n",
    "        if words.count(w) == 0:\n",
    "            words.append(w)\n",
    "\n",
    "## creates room for parameters, add more zeros as needed\n",
    "## characters in the word is set in index 2\n",
    "for k in range(len(words)):\n",
    "    words[k] = [words[k], 0, len(words[k]), 0, 0, 0, 0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## counts number of time each word occurs in the chat(set in index 1)\n",
    "def wordOccur(w, wordList):\n",
    "    c = 0\n",
    "    for k in range(len(wordList)):\n",
    "        c = c + wordList[k].count(w)\n",
    "            \n",
    "    return c\n",
    "\n",
    "def wordOccurAvg(w, b, wordList):\n",
    "    c = 0\n",
    "    for k in range(len(wordList)):\n",
    "        if wordList[k].count(w) != 0:\n",
    "            c = c + 1\n",
    "    #TODO: UPDATE IT HERE: Can't divide by 0\n",
    "    if c == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return b/c\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## counts number of capital letters per word\n",
    "def wordCaps(word):\n",
    "    c = 0\n",
    "    \n",
    "    for n in word:\n",
    "        if n.isupper():\n",
    "            c = c + 1\n",
    "    return c\n",
    "\n",
    "## counts number of lower case letters before captial \n",
    "def lowersBeforeUpper(word):\n",
    "    counter = 0\n",
    "    \n",
    "    if not word[0].isalpha():\n",
    "        word = word[1:]\n",
    "    \n",
    "    for letter in word:\n",
    "        if not letter.isupper():\n",
    "            counter += 1\n",
    "        else:\n",
    "            break\n",
    "    if counter == len(word):\n",
    "        return 0\n",
    "    else:\n",
    "        return counter\n",
    "    \n",
    "def singleWord(word, wordsl):\n",
    "    c = 0\n",
    "    for k in range(len(wordsl)):\n",
    "        if wordsl[k].count(word) > 0 and len(wordsl[k]) == 1:\n",
    "            c += 1\n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## index 1 is set to the number of occurances   \n",
    "## index 2 is set to the length of the word\n",
    "## index 3 is set to the number of letters that are capitalized\n",
    "## index 4 is set to average number of repetitions per message\n",
    "## index 5 is set to the number of lower case letters before the first capitalized letter, zero if there are no capitalizations\n",
    "## or if the entire word is capitalized\n",
    "## index 6 is set to the number of times the word appears in a message by itself.\n",
    "\n",
    "def putTogether(words, wordl):\n",
    "    for k in range(len(words)):\n",
    "        words[k][1] = wordOccur(words[k][0], wordl)\n",
    "        words[k][3] = wordCaps(words[k][0])\n",
    "        words[k][4] = wordOccurAvg(words[k][0], words[k][1], wordl)\n",
    "        words[k][5] = lowersBeforeUpper(words[k][0])\n",
    "        words[k][6] = singleWord(words[k][0], wordl)\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words1 = putTogether(words, wordl)\n",
    "fd.close()\n",
    "## Creates a list of all global emotes used by Twitch.tv and the extremely popular plug in BTTV to train the model on.\n",
    "filename2 = \"GlobalEmotes.txt\"\n",
    "\n",
    "path_to_file2 = os.path.join(base_path, filename2)\n",
    "fd2 = open(path_to_file2, 'r', encoding =\"utf8\")\n",
    "\n",
    "emotes = fd2.read().split('\\n')\n",
    "emoteList = []\n",
    "\n",
    "for k in range(len(emotes)):\n",
    "    if len(emotes[k]) >= 2:\n",
    "        emoteList.append(emotes[k])\n",
    "\n",
    "fd2.close()\n",
    "\n",
    "filename3 = \"BTTVEmotes.txt\"\n",
    "\n",
    "path_to_file3 = os.path.join(base_path, filename3)\n",
    "fd3 = open(path_to_file3, 'r', encoding =\"utf8\")\n",
    "\n",
    "bttvEmotes = fd3.read().split('\\n')\n",
    "        \n",
    "for k in range(len(bttvEmotes)):\n",
    "    bttvEmotes[k] = bttvEmotes[k].split(' ')\n",
    "    if (len(bttvEmotes[k][0]) > 1):\n",
    "        emoteList.append(bttvEmotes[k][0])\n",
    "fd3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## appends certain known channel emotes to master list of emotes. This list is far from comprehensive, and will require\n",
    "## training the model to be able to recognize such emotes. The point is for this model to be rebust and able to detect\n",
    "## future emotes that are not part of this list, as emotes are regularly added, deleted and replaced\n",
    "emoteList.append(\"admiralG\")\n",
    "emoteList.append(\"admiralW\")\n",
    "emoteList.append(\"admiralC1\")\n",
    "emoteList.append(\"admiralC2\")\n",
    "emoteList.append(\"admiralGame\")\n",
    "emoteList.append(\"admiralGa\")\n",
    "emoteList.append(\"admiralC\")\n",
    "emoteList.append(\"cpkSwag\")\n",
    "emoteList.append(\"cpkFiesta\")\n",
    "emoteList.append(\"cpkMurica\")\n",
    "emoteList.append(\"febbyW\")\n",
    "emoteList.append(\"febbyS\")\n",
    "emoteList.append(\"febbyNa\")\n",
    "emoteList.append(\"febbyWh\")\n",
    "emoteList.append(\"febbyGasm\")\n",
    "emoteList.append(\"febbyWut\")\n",
    "emoteList.append(\"febbyS1\")\n",
    "emoteList.append(\"febbyP\")\n",
    "emoteList.append(\"capES1\")\n",
    "emoteList.append(\"capES2\")\n",
    "emoteList.append(\"capES3\")\n",
    "emoteList.append(\"capCap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.08036291e-03  7.66315042e-04  9.61938498e-04  6.38333722e-04\n",
      "  4.05485940e-05 -5.57014930e-05]\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "## Converts out list of words into a 4542 x 6 array with numpy minus the initial word in index 1\n",
    "words2 = words1.copy()\n",
    "for k in range(len(words2)):            ## iterates through a copy of the word list and removes the word at index 0\n",
    "    words2[k] = deque(words2[k])\n",
    "    words2[k].popleft()\n",
    "    \n",
    "wordArray = np.asarray(words2)         \n",
    "\n",
    "## Creates a 6 x 1 Matrix to hold weights\n",
    "weights = np.random.randn(wordArray.shape[1])*0.001\n",
    "print(weights)\n",
    "\n",
    "## Creates a 4542 x 1 Matrix to hold outputs\n",
    "output = np.ones(wordArray.shape[0])\n",
    "\n",
    "for k in range(output.shape[0]):\n",
    "    if emoteList.count(words[k][0]) >= 1:\n",
    "        output[k] = 1\n",
    "    else:\n",
    "        output[k] = 0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## implementing log-likelihood function for {0, 1}\n",
    "def loglikelihood(w, X,y, alpha):\n",
    "    xbMatrix = np.dot(X, w)\n",
    "    prob = np.exp(y * xbMatrix)/(1 + np.exp(xbMatrix))\n",
    "    X = X.T\n",
    "    gradVal = np.dot(X, np.exp(xbMatrix)/(1 + np.exp(xbMatrix)))\n",
    "    gradValy = np.dot(X, y)\n",
    "    penalty = alpha/2 * np.sum(w**2)\n",
    "    gradPenalty = -alpha * w\n",
    "    return -np.sum(y * (xbMatrix) - np.log( 1 + np.exp(xbMatrix)))- (alpha / 2) * np.sum(w**2), -((gradValy - gradVal) - gradPenalty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## implementation of grad_check, gradient ascent and optimizeFn borrowed from HW1\n",
    "def grad_check(f,xy0,delta=1e-6,tolerance=1e-7):\n",
    "    f0,g0 = f(xy0)\n",
    "    p = len(xy0)\n",
    "    finite_diff = np.zeros(p)\n",
    "    gradient_correct = True\n",
    "    for i in range(p):\n",
    "        xy1 = np.copy(xy0)\n",
    "        xy2 = np.copy(xy0)\n",
    "        xy1[i] = xy1[i] - 0.5*delta\n",
    "        xy2[i] = xy2[i] + 0.5*delta\n",
    "        f1,_ = f(xy1)\n",
    "        f2,_ = f(xy2)\n",
    "        finite_diff = (f2 - f1)/(delta)\n",
    "        if (abs(finite_diff - g0[i])>tolerance):\n",
    "            print(\"Broken partial\",i,\" Finite Diff: \",finite_diff,\" Partial: \",g0[i])\n",
    "            gradient_correct = False\n",
    "    return gradient_correct\n",
    "\n",
    "g = lambda xy0: loglikelihood(xy0, wordArray, output, 1)\n",
    "grad_check( g, weights, delta=1e-6, tolerance=1e-5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_ascent(f,x,init_step,iterations):  \n",
    "    f_val,grad = f(x)                           \n",
    "    f_vals = [f_val]\n",
    "    for it in range(iterations):                \n",
    "        done = False                            \n",
    "        line_search_it = 0                      \n",
    "        step = init_step                        \n",
    "        while not done and line_search_it<100: \n",
    "            new_x = x + step*grad               \n",
    "            new_f_val,new_grad = f(new_x)      \n",
    "            if new_f_val<f_val:                 \n",
    "                step = step*0.95                \n",
    "                line_search_it += 1            \n",
    "            else:\n",
    "                done = True                     \n",
    "        \n",
    "        if not done:                            \n",
    "            print(\"Line Search failed.\")\n",
    "        else:\n",
    "            f_val = new_f_val                   \n",
    "            x = new_x\n",
    "            grad = new_grad\n",
    "            f_vals.append(f_val)\n",
    "    return f_val, x\n",
    "\n",
    "def optimizeFn( init_step, iterations, alpha, w):\n",
    "    g = lambda xy0: loglikelihood(xy0, wordArray, output, alpha)\n",
    "    f_val, update_w = gradient_ascent( g, w, init_step, iterations )\n",
    "    return f_val, update_w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3152.4983043630805, array([ 6871.97843022, 12305.81451137,   981.70587337,  2177.12895215,\n",
      "          18.39575132,    82.88372366]))\n",
      "(14073.189947869465, array([14304.45670606, 23690.66033857,  1945.96930249,  4117.20216874,\n",
      "         144.28065841,   288.91080261]))\n"
     ]
    }
   ],
   "source": [
    "print(loglikelihood(weights, wordArray, output, 300))\n",
    "f_val, update_w=optimizeFn( init_step = 1e-7, iterations=200, alpha=300, w = weights) #set init_step to 1e-4, 1e-5, 1e-6\n",
    "print(loglikelihood(update_w, wordArray, output, 300))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3891\n",
      "4440\n"
     ]
    }
   ],
   "source": [
    "def prediction(w, Xf):\n",
    "    wf = np.array(list(map(float, w)))\n",
    "    prob = np.exp(np.dot(Xf, wf))/(1 + np.exp(np.dot(Xf,wf)))\n",
    "    res = np.zeros(Xf.shape[0])\n",
    "    res[prob >= .5] = 0\n",
    "    res[prob >= .5] = 1\n",
    "    return res\n",
    "loss = 0\n",
    "\n",
    "check = prediction(weights, wordArray) ## checking number of misclassifications with untrained weights\n",
    "for k in range(len(check)):\n",
    "    if output[k] != check[k]:\n",
    "        loss += 1\n",
    "        \n",
    "print(loss)\n",
    "loss = 0\n",
    "\n",
    "check = prediction(update_w, wordArray) ## checking number of misclassifications with trained weights\n",
    "for k in range(len(check)):\n",
    "    if output[k] != check[k]:\n",
    "        loss += 1\n",
    "        \n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## It doesn't look like gradient ascent is doing very well. Let's try the other way.\n",
    "def gradient_descent(f,x,init_step,iterations):  \n",
    "    f_val,grad = f(x)                           \n",
    "    f_vals = [f_val]\n",
    "    for it in range(iterations):                \n",
    "        done = False                            \n",
    "        line_search_it = 0                      \n",
    "        step = init_step                        \n",
    "        while not done and line_search_it<100: \n",
    "            new_x = x - step*grad               \n",
    "            new_f_val,new_grad = f(new_x)      \n",
    "            if new_f_val>f_val:                 \n",
    "                step = step*0.95                \n",
    "                line_search_it += 1            \n",
    "            else:\n",
    "                done = True                     \n",
    "        \n",
    "        if not done:                            \n",
    "            print(\"Line Search failed.\")\n",
    "        else:\n",
    "            f_val = new_f_val                   \n",
    "            x = new_x\n",
    "            grad = new_grad\n",
    "            f_vals.append(f_val)\n",
    "    return f_val, x\n",
    "\n",
    "def optimizeFn( init_step, iterations, alpha, w):\n",
    "    g = lambda xy0: loglikelihood(xy0, wordArray, output, alpha)\n",
    "    f_val, update_w = gradient_descent( g, w, init_step, iterations )\n",
    "    return f_val, update_w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_val, update_w=optimizeFn( init_step = 1e-7, iterations=200, alpha=300, w = weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.08036291e-03  7.66315042e-04  9.61938498e-04  6.38333722e-04\n",
      "  4.05485940e-05 -5.57014930e-05]\n",
      "3891\n",
      "102\n"
     ]
    }
   ],
   "source": [
    "loss = 0\n",
    "print(weights)\n",
    "check = prediction(weights, wordArray) ##Checking number of misclassifications with un-trained weights\n",
    "for k in range(len(check)):\n",
    "    if output[k] != check[k]:\n",
    "        loss += 1\n",
    "        \n",
    "print(loss)\n",
    "loss = 0\n",
    "\n",
    "check = prediction(update_w, wordArray) ## Checking number of misclassifications with trained weights.\n",
    "for k in range(len(check)):\n",
    "    if output[k] != check[k]:\n",
    "        loss += 1\n",
    "        \n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n"
     ]
    }
   ],
   "source": [
    "## There we go. It looks like lower weights are better. Lets try starting with higher initial weights.\n",
    "weights2 = np.ones(wordArray.shape[1])\n",
    "f_val, update_w2=optimizeFn( init_step = 1e-5, iterations=500, alpha= 0, w = weights2)\n",
    "loss = 0\n",
    "\n",
    "check = prediction(update_w2, wordArray)\n",
    "for k in range(len(check)):\n",
    "    if output[k] != check[k]:\n",
    "        loss += 1\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pretty good. Lets try with another data set.\n",
    "filename4 = \"V.STORM VS EG2.txt\"\n",
    "\n",
    "path_to_file4 = os.path.join(base_path, filename4)\n",
    "fd4 = open(path_to_file4, 'r', encoding =\"utf8\")\n",
    "\n",
    "testList = fd4.read().split('\\n')\n",
    "\n",
    "\n",
    "for k in range(len(testList)):\n",
    "    testList[k] = testList[k].split(' ')\n",
    "    m = m + 1\n",
    "    \n",
    "wordl2 = testList.copy()\n",
    "words2 = []\n",
    "    \n",
    "## removes the timestamp and the user name, which are stored in the first two elements of each sub-array\n",
    "for k in range(len(wordl2)):\n",
    "    wordl2[k] = wordl2[k][2:]\n",
    "\n",
    "## adds each word to the words array. A word will only be added once, regardless of how many times it appears\n",
    "for k in range(len(wordl2)):\n",
    "    for w in wordl2[k]:\n",
    "        if words2.count(w) == 0:\n",
    "            words2.append(w)\n",
    "\n",
    "## creates room for parameters, add more zeros as needed\n",
    "## characters in the word is set in index 2\n",
    "for k in range(len(words2)):\n",
    "    words2[k] = [words2[k], 0, len(words2[k]), 0, 0, 0, 0]\n",
    "    \n",
    "tList = putTogether(words2, wordl2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoteList.append(\"sheriffDANCE\")\n",
    "emoteList.append(\"sheriffDANCE2\")\n",
    "emoteList.append(\"sheriffDANCE3\")\n",
    "emoteList.append(\"dayPwr\")\n",
    "emoteList.append(\"dayHot\")\n",
    "emoteList.append(\"singsingF\")\n",
    "emoteList.append(\"singsingU\")\n",
    "emoteList.append(\"singsingC\")\n",
    "emoteList.append(\"singsingK\")\n",
    "emoteList.append(\"pokiAww\")\n",
    "emoteList.append(\"pokiLUL\")\n",
    "emoteList.append(\"pokiYAY\")\n",
    "emoteList.append(\"pokiBAN\")\n",
    "emoteList.append(\"pokiPOP\")\n",
    "emoteList.append(\"pokiHug\")\n",
    "emoteList.append(\"rtzW1\")\n",
    "emoteList.append(\"rtzW2\")\n",
    "emoteList.append(\"forsenPosture\")\n",
    "emoteList.append(\"forsenPosture1\")\n",
    "emoteList.append(\"forsenPosture2\")\n",
    "emoteList.append(\"forsenTake\")\n",
    "emoteList.append(\"forsenA\")\n",
    "emoteList.append(\"forsenDED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tList2 = tList.copy()\n",
    "for k in range(len(tList2)): \n",
    "    tList2[k] = deque(tList2[k])\n",
    "    tList2[k].popleft()\n",
    "    \n",
    "wordArray2 = np.asarray(tList2)         \n",
    "\n",
    "## Creates a 4542 x 1 Matrix to hold outputs\n",
    "output2 = np.ones(wordArray2.shape[0])\n",
    "\n",
    "for k in range(output2.shape[0]):\n",
    "    if emoteList.count(words2[k][0]) >= 1:\n",
    "        output2[k] = 1\n",
    "    else:\n",
    "        output2[k] = 0   \n",
    "        \n",
    "weights2 = np.random.randn(wordArray.shape[1])*0.001 ## some random weights to test against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243\n",
      "451\n"
     ]
    }
   ],
   "source": [
    "loss = 0\n",
    "check = prediction(weights2, wordArray2)\n",
    "for k in range(len(check)):\n",
    "    if output2[k] != check[k]:\n",
    "        loss += 1\n",
    "\n",
    "print(loss)\n",
    "\n",
    "loss = 0\n",
    "\n",
    "check = prediction(update_w2, wordArray2)\n",
    "for k in range(len(check)):\n",
    "    if output2[k] != check[k]:\n",
    "        loss += 1\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13762\n"
     ]
    }
   ],
   "source": [
    "## Seems pretty good.\n",
    "print(output2.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression -- log-likelihood for $0,1$ labels\n",
    "$$\n",
    "\\renewcommand{\\xx}{\\mathbf{x}}\n",
    "\\renewcommand{\\yy}{\\mathbf{y}}\n",
    "\\renewcommand{\\zz}{\\mathbf{z}}\n",
    "\\renewcommand{\\vv}{\\mathbf{v}}\n",
    "\\renewcommand{\\bbeta}{\\boldsymbol{\\mathbf{\\beta}}}\n",
    "\\renewcommand{\\loglik}{\\mathcal{LL}}\n",
    "\\renewcommand{\\penloglik}{\\mathcal{PLL}}\n",
    "\\renewcommand{\\likelihood}{\\mathcal{L}}\n",
    "\\renewcommand{\\Data}{\\textrm{Data}}\n",
    "\\renewcommand{\\given}{ \\big| }\n",
    "\\renewcommand{\\MLE}{\\textrm{MLE}}\n",
    "\\renewcommand{\\tth}{\\textrm{th}}\n",
    "\\renewcommand{\\Gaussian}[2]{\\mathcal{N}\\left(#1,#2\\right)}\n",
    "\\renewcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\renewcommand{\\ones}{\\mathbf{1}}\n",
    "\\renewcommand{\\diag}[1]{\\textrm{diag}\\left( #1 \\right)}\n",
    "\\renewcommand{\\sigmoid}[1]{\\sigma\\left(#1\\right)}\n",
    "\\renewcommand{\\myexp}[1]{\\exp\\left\\{#1\\right\\}}\n",
    "\\renewcommand{\\mylog}[1]{\\log\\left\\{#1\\right\\}}\n",
    "$$\n",
    "\n",
    "\n",
    "Probability of a single sample is when $y \\in \\{0,1\\}$:\n",
    "$$\n",
    "p(y\\given\\xx,\\beta_0,\\beta) = \\frac{\\myexp{y(\\beta_0 + \\xx^T\\beta)}}{1 + \\myexp{(\\beta_0 + \\xx^T\\beta)}}\n",
    "$$\n",
    "\n",
    "Likelihood function is:\n",
    "$$\n",
    "\\likelihood(\\beta_0,\\beta\\given\\yy,\\xx) = \\prod_i \\frac{\\myexp{y_i(\\beta_0 + \\xx_i^T\\beta)}}{1 + \\myexp{(\\beta_0 + \\xx^T\\beta)}}\n",
    "$$\n",
    "\n",
    "Log-likelihood function is:\n",
    "$$\n",
    "\\loglik(\\beta_0,\\beta\\given\\yy,\\xx) = \\sum_i y_i(\\beta_0 + \\xx_i^T\\beta) - \\log\\left\\{1 + \\myexp{(\\beta_0 + \\xx_i^T\\beta)} \\right\\}\n",
    "$$\n",
    "\n",
    "Ridge regularized log-likelihood:\n",
    "$$\n",
    "\\penloglik(\\beta_0,\\beta\\given\\yy,\\xx) =  \\sum_i y_i(\\beta_0 + \\xx_i^T\\beta) - \\log\\left\\{1 + \\myexp{(\\beta_0 + \\xx_i^T\\beta)} \\right\\} - \\frac{\\lambda}{2}\\norm{\\beta}^2\n",
    "$$\n",
    "\n",
    "1. $ L(\\mathbf{w}) =  \\prod_i \\frac{\\myexp{y_i(\\mathbf{w} \\cdot \\mathbf{x_i})}}{1 + \\myexp{(\\mathbf{w} \\cdot \\mathbf{x_i})}}$\n",
    "OR $ L(\\mathbf{w}) = \\prod_i \\frac{\\myexp{y_i(\\beta_0 + \\xx_i^T\\beta)}}{1 + \\myexp{(\\beta_0 + \\xx^T\\beta)}} $\n",
    "\n",
    "\n",
    "2. $ LL(\\mathbf{w}) =  \\sum_i y_i(\\mathbf{w} \\cdot \\mathbf{x_i}) - \\log\\left\\{1 + \\myexp{(\\mathbf{w} \\cdot \\mathbf{x_i})} \\right\\} $\n",
    "OR $ LL(\\mathbf{w}) =  \\sum_i y_i(\\beta_0 + \\xx_i^T\\beta) - \\log\\left\\{1 + \\myexp{(\\beta_0 + \\xx_i^T\\beta)} \\right\\} $\n",
    "\n",
    "\n",
    "3. $ PLL(\\mathbf{w}) = \\sum_i y_i(\\mathbf{w} \\cdot \\mathbf{x_i}) - \\log\\left\\{1 + \\myexp{(\\mathbf{w} \\cdot \\mathbf{x_i})} \\right\\} - \\frac{\\alpha}{2}\\norm{w_j}^2 $\n",
    "OR $ PLL(\\mathbf{w}) = \\sum_i y_i(\\beta_0 + \\xx_i^T\\beta) - \\log\\left\\{1 + \\myexp{(\\beta_0 + \\xx_i^T\\beta)} \\right\\} - \\frac{\\lambda}{2}\\norm{\\beta}^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now feed text message data for clip classifier\n",
    "## Todo: 1. A list of cleaned data (dimension N*3)\n",
    "##       2. Create a randomly generated weights list and a list for holding outputs (of the same dimension)\n",
    "##       3. Run OptimizeFn (use either gradient_ascent OR gradient_descent) to get an updated weights\n",
    "##       4. Run prediction on both weights\n",
    "##       5. Write a function to count loss -- compare it with output (in this case, more than average chats & unique users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      "35\n",
      "36\n",
      "51\n",
      "52\n",
      "84\n",
      "85\n",
      "86\n",
      "94\n",
      "95\n",
      "96\n",
      "115\n",
      "116\n",
      "117\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "(192,)\n"
     ]
    }
   ],
   "source": [
    "#Here is where we create our manually selected highlights for training the clip classifier\n",
    "\n",
    "output2 = np.zeros(192) ## replace wordArray 2 with the data matrix and output2 with the output matrix for data set 1\n",
    "\n",
    "manualC = output2\n",
    "manualC[35] = 1\n",
    "manualC[36] = 1\n",
    "manualC[51] = 1\n",
    "manualC[52] = 1\n",
    "manualC[84] = 1\n",
    "manualC[85] = 1\n",
    "manualC[86] = 1\n",
    "manualC[94] = 1\n",
    "manualC[95] = 1\n",
    "manualC[96] = 1\n",
    "manualC[115] = 1\n",
    "manualC[116] = 1\n",
    "manualC[117] = 1\n",
    "index = [167, 168, 169, 170, 171, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185]\n",
    "manualC[index] = 1\n",
    "print(manualC)\n",
    "for k in range(len(manualC)):\n",
    "    if manualC[k] == 1:\n",
    "        print(k);\n",
    "print(manualC.shape)\n",
    "\n",
    "np.savetxt(\"V.STORM VS EG manual highlights.txt\", manualC)\n",
    "        \n",
    "#manualC2 = output3\n",
    "#index = [17, 18, 19, 20, 42, 43, 44, 55, 56, 57, 58, 94, 95, 96, 97, 110, 111, 112, 113, 125, 126, 127, 128, 143, 144, 145, 146, 166, 167, 168, 169, 170, 171, 172]\n",
    "#manualC2[index] = 1\n",
    "\n",
    "#np.savetxt(\"V.STORM VS EG2 manual highlights.txt\", manualC)\n",
    "\n",
    "#manualC3 = output4\n",
    "#index = [30, 31, 42, 43, 50, 51, 63, 64, 73, 74, 75, 85, 86, 87, 88, 115, 116, 125, 126, 137, 138, 139, 140]\n",
    "#manualC3[index] = 1\n",
    "\n",
    "#np.savetxt(\"V.STORM VS EG3 manual highlights.txt\", manualC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 192)\n"
     ]
    }
   ],
   "source": [
    "## 1. Load data from cleaned clip data \n",
    "matrix = np.loadtxt('clipData.txt') #, dtype=int\n",
    "\n",
    "print(matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights shape  (4,)\n",
      "weights:  [-5.29989725e-04 -1.16536353e-03  1.11527741e-03  3.50235225e-05]\n",
      "highlights.shape:  (192,)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      "data shape:  (192, 4)\n",
      "bxMatrix.shape (192,)\n"
     ]
    }
   ],
   "source": [
    "## 2a. Creates a 3 x 1 Matrix to hold weights\n",
    "weights = np.random.randn(matrix.shape[0])*0.001  #TODO: 1 vs 0 here??\n",
    "print(\"weights shape \", weights.shape)\n",
    "print(\"weights: \", weights)\n",
    "\n",
    "## 2b. Creates a N x 1 Matrix for the highlights\n",
    "#highlights = np.ones(matrix.shape[1])\n",
    "highlights = np.loadtxt('V.STORM VS EG manual highlights.txt')\n",
    "print(\"highlights.shape: \", highlights.shape)\n",
    "\n",
    "print(highlights)\n",
    "\n",
    "#TODO: Check shape\n",
    "\n",
    "data = matrix.copy()\n",
    "data = data.T\n",
    "\n",
    "print(\"data shape: \", data.shape)\n",
    "\n",
    "bxMatrix = np.dot(weights, matrix)\n",
    "print(\"bxMatrix.shape\", bxMatrix.shape)\n",
    "ybxMatrix = highlights * bxMatrix\n",
    "\n",
    "xbMatrix = np.dot(matrix.T, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.09207488e-02 8.75444443e-02 5.87282038e-02 3.48837075e-05]\n",
      "527.6651904681283\n"
     ]
    }
   ],
   "source": [
    "## 3a. Run OptimizeFn using gradient_ascent \n",
    "\n",
    "def loglikelihoodNew(w, X, y, alpha):\n",
    "    xbMatrix = np.dot(X, w) #TODO: (w,X) or (X,w) or (X.T, w)\n",
    "    prob = np.exp(y * xbMatrix)/(1 + np.exp(xbMatrix))\n",
    "    X = X.T\n",
    "    gradVal = np.dot(X, np.exp(xbMatrix)/(1 + np.exp(xbMatrix)))\n",
    "    gradValy = np.dot(X, y)\n",
    "    penalty = alpha/2 * np.sum(w**2)\n",
    "    gradPenalty = -alpha * w\n",
    "    return -np.sum(y * (xbMatrix) - np.log( 1 + np.exp(xbMatrix)))- (alpha / 2) * np.sum(w**2), -((gradValy - gradVal) - gradPenalty)\n",
    "\n",
    "def optimizeFnAsc( init_step, iterations, alpha, w, data, highlight):\n",
    "    g = lambda xy0: loglikelihoodNew(xy0, data, highlight, alpha)\n",
    "    f_val, update_asc_w = gradient_ascent( g, w, init_step, iterations )\n",
    "    return f_val, update_asc_w\n",
    "\n",
    "f_val_asc, update_asc_w=optimizeFnAsc( init_step = 1e-7, iterations=400, alpha=100, w = weights, data=data, highlight=highlights)\n",
    "\n",
    "print(update_asc_w)\n",
    "print(f_val_asc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.01286280e-02 -1.88358225e-02 -7.89851620e-03  3.52342922e-05]\n"
     ]
    }
   ],
   "source": [
    "## 3b. Run OptimizaFn using gradient_descent\n",
    "def optimizeFnDes( init_step, iterations, alpha, w, data, highlight):\n",
    "    g = lambda xy0: loglikelihoodNew(xy0, data, highlight, alpha)\n",
    "    f_val, update_des_w = gradient_descent( g, w, init_step, iterations )\n",
    "    return f_val, update_des_w\n",
    "\n",
    "f_val_des, update_des_w=optimizeFnDes( init_step = 1e-7, iterations=200, alpha=300, w = weights, data=data, highlight=highlights)\n",
    "\n",
    "print(update_des_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.T shape:  (192, 4)\n",
      "prediction shape:  (192,)\n",
      "[1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "## 4a. Run prediction on untrained weights\n",
    "print(\"data.T shape: \", data.shape)\n",
    "\n",
    "untrained_predictions = prediction(weights, data)\n",
    "\n",
    "print(\"prediction shape: \", untrained_predictions.shape)\n",
    "print(untrained_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192,)\n",
      "192\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "(192,)\n",
      "192\n",
      "[1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "## 4b. Run prediction on trained weights\n",
    "trained_asc_predictions = prediction(update_asc_w, data)\n",
    "\n",
    "print(trained_asc_predictions.shape)\n",
    "print(len(trained_asc_predictions))\n",
    "\n",
    "print(trained_asc_predictions)\n",
    "\n",
    "## TODO: do it on updated weights using gradient descent\n",
    "trained_des_predictions = prediction(update_des_w, data)\n",
    "\n",
    "print(trained_des_predictions.shape)\n",
    "print(len(trained_des_predictions))\n",
    "\n",
    "print(trained_des_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for untrained data:  0.3385416666666667\n",
      "accuracy for gradient asc trained data:  0.8385416666666666\n",
      "accuracy for gradient des trained data:  0.3229166666666667\n"
     ]
    }
   ],
   "source": [
    "## 5. Compare it against highlights\n",
    "def getAccuracy(highlights, predictions):\n",
    "    loss = 0\n",
    "    for k in range(len(predictions)):\n",
    "        if highlights[k] != predictions[k]:\n",
    "            loss += 1\n",
    "    return loss/len(highlights)\n",
    "\n",
    "for k in range(len(trained_des_predictions)):\n",
    "    if highlights[k] != trained_des_predictions[k]:\n",
    "        loss += 1\n",
    "\n",
    "lossUn = getAccuracy(highlights, untrained_predictions)\n",
    "lossAsc = getAccuracy(highlights, trained_asc_predictions)\n",
    "lossDes = getAccuracy(highlights, trained_des_predictions)\n",
    "print(\"accuracy for untrained data: \", lossUn)\n",
    "print(\"accuracy for gradient asc trained data: \", lossAsc)\n",
    "print(\"accuracy for gradient des trained data: \", lossDes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: run it against the manually classified highlights instead of highlights resulting from the upper 25% of all parameters\n",
    "# 1. Manually identify the highlight minutes\n",
    "# 2. Create 0 and 1 matrix for the highlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's combine the two models \n",
    "# 1. In every minute: run emoClassifier on all the worlds using the 6 parameters and the trained weights\n",
    "# 2. Multiply the wordcount by y (output from 1.) to get the total amount of emos in each minute\n",
    "# 3. Use either the result from 2, or the percentage of emos (divide 2 by total amount of words) as the 4th parameter\n",
    "# 4. With the 4th parameter, run the predictions again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Function for Running emoModel on the words of each minute \n",
    "# (given a double array of words (row: each comment, column: all the words in this comment))\n",
    "# Here we are returning the total count of emos in this block (a minute)\n",
    "def runEmoModel(wordsList):\n",
    "    words = []\n",
    "\n",
    "    ## adds each word to the words array. A word will only be added once, regardless of how many times it appears\n",
    "    for k in range(len(wordsList)):\n",
    "        for w in wordsList[k]:\n",
    "            if words.count(w) == 0:\n",
    "                words.append(w)\n",
    "\n",
    "    ## creates room for parameters, add more zeros as needed\n",
    "    ## characters in the word is set in index 2\n",
    "    for k in range(len(words)):\n",
    "        words[k] = [words[k], 0, len(words[k]), 0, 0, 0, 0]\n",
    "\n",
    "    weightedWords = putTogether(words, wordsList)\n",
    "    ## index 1 is set to the number of occurances   \n",
    "    ## index 2 is set to the length of the word\n",
    "    ## index 3 is set to the number of letters that are capitalized\n",
    "    ## index 4 is set to average number of repetitions per message\n",
    "    ## index 5 is set to the number of lower case letters before the first capitalized letter, zero if there are no capitalizations\n",
    "    ## or if the entire word is capitalized\n",
    "    ## index 6 is set to the number of times the word appears in a message by itself.\n",
    "    \n",
    "    #print(\"weightedWords\", weightedWords)\n",
    "\n",
    "    #Remove the first column (word String)\n",
    "    for k in range(len(weightedWords)): \n",
    "        weightedWords[k] = deque(weightedWords[k])\n",
    "        weightedWords[k].popleft()\n",
    "    \n",
    "    wordsMatrix = np.asarray(weightedWords) \n",
    "      \n",
    "    y = prediction(update_w2, wordsMatrix)\n",
    "    #print(\"here is the y\", y)\n",
    "    #print(np.count_nonzero(y))\n",
    "    \n",
    "    #TODO: now we have the prediction (0 and 1) of each unique word\n",
    "    #Need to multiply it by the words occurrence \n",
    "    count = 0\n",
    "    for k in range(len(y)):\n",
    "        if y[k] == 1:\n",
    "            occur = wordsMatrix[k][0]\n",
    "            count += occur\n",
    "    \n",
    "    return count\n",
    "\n",
    "\n",
    "wordsListTesting = [['will', 'NA', 'finals', 'be', 'casted', 'here', '?'], ['oh', 'nvm']]\n",
    "\n",
    "words = [['Yooo']]\n",
    "\n",
    "runEmoModel(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "1 :\t 17.0 \t 15.0 \t 17.0 0.0\n",
      "2 :\t 31.0 \t 22.774193548387096 \t 29.0 0.0\n",
      "3 :\t 10.0 \t 59.2 \t 10.0 0.0\n",
      "4 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "5 :\t 12.0 \t 25.583333333333332 \t 12.0 0.0\n",
      "6 :\t 21.0 \t 17.857142857142858 \t 19.0 0.0\n",
      "7 :\t 13.0 \t 25.692307692307693 \t 13.0 0.0\n",
      "8 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "9 :\t 10.0 \t 18.5 \t 10.0 0.0\n",
      "10 :\t 37.0 \t 17.81081081081081 \t 30.0 0.0\n",
      "11 :\t 23.0 \t 19.47826086956522 \t 22.0 0.0\n",
      "12 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "13 :\t 15.0 \t 23.933333333333334 \t 15.0 0.0\n",
      "14 :\t 35.0 \t 25.457142857142856 \t 29.0 0.0\n",
      "15 :\t 20.0 \t 16.0 \t 18.0 0.0\n",
      "16 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "17 :\t 10.0 \t 16.2 \t 9.0 0.0\n",
      "18 :\t 21.0 \t 19.571428571428573 \t 20.0 0.0\n",
      "19 :\t 10.0 \t 28.3 \t 10.0 0.0\n",
      "20 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "21 :\t 10.0 \t 31.5 \t 10.0 0.0\n",
      "22 :\t 14.0 \t 25.357142857142858 \t 14.0 0.0\n",
      "23 :\t 31.0 \t 19.29032258064516 \t 30.0 0.0\n",
      "24 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "25 :\t 7.0 \t 30.714285714285715 \t 7.0 0.0\n",
      "26 :\t 24.0 \t 21.166666666666668 \t 20.0 0.0\n",
      "27 :\t 4.0 \t 15.5 \t 4.0 0.0\n",
      "28 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "29 :\t 6.0 \t 28.833333333333332 \t 6.0 0.0\n",
      "30 :\t 12.0 \t 33.833333333333336 \t 11.0 0.0\n",
      "31 :\t 8.0 \t 51.5 \t 8.0 0.0\n",
      "32 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "33 :\t 11.0 \t 14.545454545454545 \t 11.0 0.0\n",
      "34 :\t 20.0 \t 23.9 \t 19.0 0.0\n",
      "35 :\t 14.0 \t 14.428571428571429 \t 14.0 0.0\n",
      "36 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "37 :\t 6.0 \t 18.333333333333332 \t 6.0 0.0\n",
      "38 :\t 14.0 \t 34.92857142857143 \t 13.0 0.0\n",
      "39 :\t 8.0 \t 15.125 \t 7.0 0.0\n",
      "40 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "41 :\t 9.0 \t 14.444444444444445 \t 9.0 0.0\n",
      "42 :\t 33.0 \t 11.242424242424242 \t 29.0 0.0\n",
      "43 :\t 10.0 \t 29.5 \t 10.0 0.0\n",
      "44 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "45 :\t 5.0 \t 8.6 \t 5.0 0.0\n",
      "46 :\t 14.0 \t 20.642857142857142 \t 14.0 0.0\n",
      "47 :\t 5.0 \t 13.0 \t 5.0 0.0\n",
      "48 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "49 :\t 8.0 \t 12.25 \t 8.0 0.0\n",
      "50 :\t 13.0 \t 10.692307692307692 \t 12.0 0.0\n",
      "51 :\t 24.0 \t 10.458333333333334 \t 24.0 0.0\n",
      "52 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "53 :\t 39.0 \t 11.666666666666666 \t 37.0 0.0\n",
      "54 :\t 36.0 \t 12.416666666666666 \t 34.0 0.0\n",
      "55 :\t 17.0 \t 25.0 \t 17.0 0.0\n",
      "56 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "57 :\t 14.0 \t 24.285714285714285 \t 14.0 0.0\n",
      "58 :\t 18.0 \t 18.666666666666668 \t 16.0 0.0\n",
      "59 :\t 8.0 \t 30.75 \t 8.0 0.0\n",
      "60 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "61 :\t 15.0 \t 16.333333333333332 \t 14.0 0.0\n",
      "62 :\t 17.0 \t 28.58823529411765 \t 16.0 0.0\n",
      "63 :\t 3.0 \t 27.333333333333332 \t 3.0 0.0\n",
      "64 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "65 :\t 10.0 \t 33.4 \t 9.0 0.0\n",
      "66 :\t 14.0 \t 25.214285714285715 \t 14.0 0.0\n",
      "67 :\t 2.0 \t 33.5 \t 2.0 0.0\n",
      "68 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "69 :\t 15.0 \t 11.8 \t 15.0 0.0\n",
      "70 :\t 38.0 \t 21.394736842105264 \t 35.0 0.0\n",
      "71 :\t 14.0 \t 14.785714285714286 \t 14.0 0.0\n",
      "72 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "73 :\t 11.0 \t 25.727272727272727 \t 11.0 0.0\n",
      "74 :\t 47.0 \t 18.340425531914892 \t 42.0 0.0\n",
      "75 :\t 27.0 \t 15.851851851851851 \t 26.0 0.0\n",
      "76 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "77 :\t 16.0 \t 16.125 \t 15.0 0.0\n",
      "78 :\t 32.0 \t 20.8125 \t 32.0 0.0\n",
      "79 :\t 12.0 \t 17.25 \t 12.0 0.0\n",
      "80 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "81 :\t 15.0 \t 36.93333333333333 \t 15.0 0.0\n",
      "82 :\t 20.0 \t 45.1 \t 18.0 0.0\n",
      "83 :\t 5.0 \t 19.2 \t 5.0 0.0\n",
      "84 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "85 :\t 25.0 \t 9.52 \t 25.0 0.0\n",
      "86 :\t 44.0 \t 14.613636363636363 \t 36.0 0.0\n",
      "87 :\t 12.0 \t 19.25 \t 12.0 0.0\n",
      "88 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "89 :\t 10.0 \t 32.6 \t 10.0 0.0\n",
      "90 :\t 19.0 \t 22.42105263157895 \t 19.0 0.0\n",
      "91 :\t 7.0 \t 28.571428571428573 \t 7.0 0.0\n",
      "92 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "93 :\t 7.0 \t 18.571428571428573 \t 7.0 0.0\n",
      "94 :\t 16.0 \t 27.8125 \t 16.0 0.0\n",
      "95 :\t 15.0 \t 13.533333333333333 \t 15.0 0.0\n",
      "96 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "97 :\t 35.0 \t 17.65714285714286 \t 32.0 0.0\n",
      "98 :\t 42.0 \t 19.904761904761905 \t 38.0 0.0\n",
      "99 :\t 7.0 \t 28.857142857142858 \t 7.0 0.0\n",
      "100 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "101 :\t 8.0 \t 35.75 \t 8.0 0.0\n",
      "102 :\t 20.0 \t 38.75 \t 19.0 0.0\n",
      "103 :\t 3.0 \t 28.0 \t 3.0 0.0\n",
      "104 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "105 :\t 23.0 \t 12.782608695652174 \t 22.0 0.0\n",
      "106 :\t 28.0 \t 17.071428571428573 \t 26.0 0.0\n",
      "107 :\t 16.0 \t 13.4375 \t 16.0 0.0\n",
      "108 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "109 :\t 9.0 \t 23.11111111111111 \t 9.0 0.0\n",
      "110 :\t 31.0 \t 19.032258064516128 \t 28.0 0.0\n",
      "111 :\t 12.0 \t 18.5 \t 12.0 0.0\n",
      "112 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "113 :\t 6.0 \t 43.5 \t 5.0 0.0\n",
      "114 :\t 16.0 \t 35.8125 \t 16.0 0.0\n",
      "115 :\t 14.0 \t 28.571428571428573 \t 14.0 0.0\n",
      "116 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "117 :\t 24.0 \t 18.375 \t 22.0 0.0\n",
      "118 :\t 40.0 \t 24.325 \t 38.0 0.0\n",
      "119 :\t 17.0 \t 18.823529411764707 \t 16.0 0.0\n",
      "120 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "121 :\t 10.0 \t 41.0 \t 10.0 0.0\n",
      "122 :\t 25.0 \t 25.12 \t 23.0 0.0\n",
      "123 :\t 8.0 \t 23.0 \t 8.0 0.0\n",
      "124 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "125 :\t 9.0 \t 20.666666666666668 \t 9.0 0.0\n",
      "126 :\t 29.0 \t 18.551724137931036 \t 24.0 0.0\n",
      "127 :\t 8.0 \t 12.125 \t 8.0 0.0\n",
      "128 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "129 :\t 11.0 \t 18.454545454545453 \t 11.0 0.0\n",
      "130 :\t 28.0 \t 21.964285714285715 \t 26.0 0.0\n",
      "131 :\t 8.0 \t 17.625 \t 8.0 0.0\n",
      "132 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "133 :\t 13.0 \t 32.07692307692308 \t 13.0 0.0\n",
      "134 :\t 29.0 \t 21.310344827586206 \t 23.0 0.0\n",
      "135 :\t 9.0 \t 32.111111111111114 \t 9.0 0.0\n",
      "136 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "137 :\t 16.0 \t 10.0 \t 16.0 0.0\n",
      "138 :\t 26.0 \t 16.96153846153846 \t 22.0 0.0\n",
      "139 :\t 15.0 \t 30.066666666666666 \t 15.0 0.0\n",
      "140 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "141 :\t 12.0 \t 33.833333333333336 \t 10.0 0.0\n",
      "142 :\t 17.0 \t 42.94117647058823 \t 14.0 0.0\n",
      "143 :\t 9.0 \t 25.88888888888889 \t 8.0 0.0\n",
      "144 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "145 :\t 11.0 \t 22.454545454545453 \t 10.0 0.0\n",
      "146 :\t 16.0 \t 36.6875 \t 14.0 0.0\n",
      "147 :\t 12.0 \t 33.166666666666664 \t 11.0 0.0\n",
      "148 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "149 :\t 27.0 \t 16.37037037037037 \t 26.0 0.0\n",
      "150 :\t 36.0 \t 14.027777777777779 \t 32.0 0.0\n",
      "151 :\t 8.0 \t 21.75 \t 8.0 0.0\n",
      "152 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "153 :\t 7.0 \t 44.857142857142854 \t 6.0 0.0\n",
      "154 :\t 16.0 \t 40.875 \t 15.0 0.0\n",
      "155 :\t 8.0 \t 38.125 \t 8.0 0.0\n",
      "156 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "157 :\t 13.0 \t 41.61538461538461 \t 13.0 0.0\n",
      "158 :\t 17.0 \t 26.529411764705884 \t 16.0 0.0\n",
      "159 :\t 7.0 \t 31.571428571428573 \t 7.0 0.0\n",
      "160 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "161 :\t 12.0 \t 23.833333333333332 \t 12.0 0.0\n",
      "162 :\t 38.0 \t 16.657894736842106 \t 37.0 0.0\n",
      "163 :\t 5.0 \t 14.2 \t 5.0 0.0\n",
      "164 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "165 :\t 21.0 \t 15.380952380952381 \t 19.0 0.0\n",
      "166 :\t 29.0 \t 20.06896551724138 \t 28.0 0.0\n",
      "167 :\t 12.0 \t 24.5 \t 12.0 0.0\n",
      "168 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "169 :\t 11.0 \t 9.727272727272727 \t 11.0 0.0\n",
      "170 :\t 45.0 \t 18.244444444444444 \t 40.0 0.0\n",
      "171 :\t 18.0 \t 18.72222222222222 \t 16.0 0.0\n",
      "172 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "173 :\t 15.0 \t 22.866666666666667 \t 15.0 0.0\n",
      "174 :\t 40.0 \t 18.55 \t 37.0 0.0\n",
      "175 :\t 14.0 \t 7.642857142857143 \t 14.0 0.0\n",
      "176 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "177 :\t 21.0 \t 27.428571428571427 \t 21.0 0.0\n",
      "178 :\t 56.0 \t 15.982142857142858 \t 53.0 0.0\n",
      "179 :\t 10.0 \t 34.5 \t 10.0 0.0\n",
      "180 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "181 :\t 16.0 \t 19.1875 \t 14.0 0.0\n",
      "182 :\t 29.0 \t 23.413793103448278 \t 27.0 0.0\n",
      "183 :\t 15.0 \t 14.866666666666667 \t 15.0 0.0\n",
      "184 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "185 :\t 14.0 \t 20.785714285714285 \t 14.0 0.0\n",
      "186 :\t 49.0 \t 26.26530612244898 \t 43.0 0.0\n",
      "187 :\t 14.0 \t 28.571428571428573 \t 14.0 0.0\n",
      "188 :\t 0.0 \t 0.0 \t 0.0 0.0\n",
      "189 :\t 13.0 \t 27.0 \t 13.0 0.0\n",
      "190 :\t 16.0 \t 21.1875 \t 16.0 0.0\n",
      "191 :\t 0.0 \t 0.0 \t 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "## loads text file into python\n",
    "base_path = \"C:/Users/mckelly/Desktop/562/FinalProject\"\n",
    "#base_path = \"/Users/ami_zou/Desktop/COMP562_Project\"\n",
    "filename = \"V.STORM VS EG.txt\"\n",
    "\n",
    "path_to_file = os.path.join(base_path, filename)\n",
    "fd = open(path_to_file, 'r', encoding =\"utf8\")\n",
    "\n",
    "## loads the text file into a double array with each message being an index in the first array and each word in the message\n",
    "## as an index in the second array\n",
    "lines = fd.read().split('\\n')\n",
    "\n",
    "# timestamps = ['h:mm:ss', 'h:mm:ss']\n",
    "timestamps = []\n",
    "# users = ['username for message 0', 'username for message 1',...]\n",
    "users = []\n",
    "# messages = ['message 0 without timestamp or user', 'message 1 without timestamp or user'...]\n",
    "messages = []\n",
    "\n",
    "\n",
    "for line in lines:\n",
    "    \n",
    "    timestamps.append(line[1:8])\n",
    "    \n",
    "    #linestr = create string out of the line\n",
    "    linestr = ''.join(line)\n",
    "    \n",
    "    #split strings into users and messages based on '<' and '>' characters\n",
    "    users.append(''.join(linestr[linestr.find('<')+1:linestr.find('>')-1]))\n",
    "    messages.append(linestr[linestr.find('>')+2:])\n",
    "    \n",
    "#trim empty line that is at the end of each file\n",
    "timestamps = timestamps[0:len(timestamps)-1]\n",
    "messages = messages[0:len(messages)-1]\n",
    "users = users[0:len(users)-1]\n",
    "\n",
    "#get last timestamp\n",
    "last_timestamp = timestamps[len(timestamps)-1]\n",
    "\n",
    "#convert last time stamp into number of rows for data matrix\n",
    "#we need one row for each minute of the stream, even if there is no activity in that minute\n",
    "\n",
    "#TODO: change it to 15sec!!!\n",
    "num_of_rows = ((int(last_timestamp[0])*60 + int(last_timestamp[2:4])) + 1)*4\n",
    "\n",
    "# number of columns in our data matrix\n",
    "num_of_columns = 4\n",
    "\n",
    "#initialize matrix to 0. using floats to get average length of message\n",
    "matrix = np.zeros((num_of_columns, num_of_rows))\n",
    "\n",
    "# users_per_minutes = { key = minute : value = list of distinct users that spoke in that minute}\n",
    "users_per_minute = dict()\n",
    "\n",
    "# messages_per_minute = { key = minute : value = list of messages in that minute}\n",
    "messages_per_minute = dict()\n",
    "\n",
    "# words_per_minute = { key = minue : value = list of the words in that minue}\n",
    "words_per_minute = dict()\n",
    "\n",
    "for i in range(0, len(timestamps)):\n",
    "    \n",
    "    # convert timestamp into index of matrix\n",
    "    hour = int(timestamps[i][0])\n",
    "    minute = int(timestamps[i][2:4])\n",
    "    sec15 = int(timestamps[i][5:7])\n",
    "    index = (hour*60 + minute)*4\n",
    "    \n",
    "    # Update index every 15 seconds\n",
    "    if(sec15 <= 30 and sec15 > 15):\n",
    "        index += 1\n",
    "    elif(sec15 <= 45):\n",
    "        index += 2\n",
    "    elif(sec15 <= 59):\n",
    "        index += 3\n",
    "    \n",
    "    # column [0] is just a counter of how many messages were in that minute\n",
    "    matrix[0][index] +=1\n",
    "    \n",
    "    # building messages_per_minute dictionary...\n",
    "    if index in messages_per_minute.keys():\n",
    "        messages_per_minute[index].append(''.join(messages[i]))\n",
    "    else:\n",
    "        messages_per_minute[index] = list()\n",
    "        messages_per_minute[index].append(''.join(messages[i]))  \n",
    "        \n",
    "    # building words_per_minute dictionary...\n",
    "    words = messages[i].split()\n",
    "    if index in words_per_minute.keys():\n",
    "        words_per_minute[index].append(words)\n",
    "    else:\n",
    "        words_per_minute[index] = list()\n",
    "        words_per_minute[index].append(words) #messages[i].split()\n",
    "    \n",
    "    # building users_per_minute dictionary...\n",
    "    if index in users_per_minute.keys():\n",
    "        if (''.join(users[i])) not in users_per_minute[index]:\n",
    "            users_per_minute[index].append(''.join(users[i]))\n",
    "    else:\n",
    "        users_per_minute[index] = list()\n",
    "        users_per_minute[index].append(''.join(users[i]))\n",
    "    \n",
    "       \n",
    "# convert dictionaries above into matrix\n",
    "for i in range(0, len(matrix[2])):\n",
    "    \n",
    "    # calculate avg length of message before inserting into matrix\n",
    "    if i in messages_per_minute.keys():\n",
    "        tmp = 0\n",
    "        for x in messages_per_minute[i]:\n",
    "            tmp += len(x)\n",
    "        \n",
    "        matrix[1][i] = tmp / len(messages_per_minute[i])\n",
    "        \n",
    "    # already trimmed duplicate users, so just insert length of users into matrix\n",
    "    if i in users_per_minute.keys():\n",
    "        matrix[2][i] = int(len(users_per_minute[i]))\n",
    "        \n",
    "    # feed word lists to the emoModel\n",
    "#    if i in words_per_minute.keys():\n",
    "#        words = words_per_minute[i]\n",
    "#        #print(\"words for this minute \", words)\n",
    "#        emoC = runEmoModel(words)\n",
    "#        matrix[3][i] = emoC\n",
    "\n",
    "#saves matrix to a txt file\n",
    "np.savetxt('clipData.txt', matrix) #, dtype=int\n",
    "\n",
    "\n",
    "for i in range(0, len(matrix[0])):\n",
    "    print(i,\":\\t\", matrix[0][i], \"\\t\", matrix[1][i], \"\\t\", matrix[2][i], matrix[3][i])\n",
    "# matrix formatting\n",
    "# each row corresponds to one minute of the chatlog, starting at minute 0\n",
    "# column [0] = how many messages were in that minute\n",
    "# column [1] = is average length in characters of each message in that minute\n",
    "# column [2] = is number of users in that minute\n",
    "# column [3] = the number/percentage of emos in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights shape  (4,)\n",
      "weights:  [-0.00012025 -0.00164312 -0.00100848 -0.0014599 ]\n",
      "highlights.shape:  (298,)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1.\n",
      " 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0.\n",
      " 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 0. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
      "dataNew shape:  (192, 4)\n"
     ]
    }
   ],
   "source": [
    "## Now with the 4th parameter, we can run all the predictions again\":\n",
    "\n",
    "## 2a. Creates a 4 x 1 Matrix to hold weights\n",
    "weightsNew = np.random.randn(matrix.shape[0])*0.001  #TODO: 1 vs 0 here??\n",
    "print(\"weights shape \", weightsNew.shape)\n",
    "print(\"weights: \", weightsNew)\n",
    "\n",
    "## 2b. Creates a N x 1 Matrix for the highlights\n",
    "#highlights = np.ones(matrix.shape[1])\n",
    "highlightsNew = np.loadtxt('manual_highlights.txt')\n",
    "print(\"highlights.shape: \", highlightsNew.shape)\n",
    "\n",
    "print(highlightsNew)\n",
    "\n",
    "#TODO: Check shape\n",
    "\n",
    "dataNew = matrix.copy()\n",
    "dataNew = dataNew.T\n",
    "\n",
    "print(\"dataNew shape: \", dataNew.shape)\n",
    "\n",
    "#bxMatrix = np.dot(weights, matrix)\n",
    "#ybxMatrix = highlights * bxMatrix\n",
    "\n",
    "#xbMatrix = np.dot(matrix.T, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update_asc_w_new:  [6.09207488e-02 8.75444443e-02 5.87282038e-02 3.48837075e-05]\n",
      "f_val_asc_new:  527.6651904681283\n",
      "update_des_w_new:  [-1.01286280e-02 -1.88358225e-02 -7.89851620e-03  3.52342922e-05]\n",
      "f_val_des_new:  106.50915893901812\n",
      "prediction_new shape:  (192,)\n",
      "untrain_new:  [1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1.]\n",
      "(192,)\n",
      "192\n",
      "asc_new:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "(192,)\n",
      "192\n",
      "des_new:  [1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1.]\n",
      "accuracy for untrained_new data:  0.3385416666666667\n",
      "accuracy for gradient asc_new trained data:  0.8385416666666666\n",
      "accuracy for gradient des_new trained data:  0.3229166666666667\n"
     ]
    }
   ],
   "source": [
    "## 3a. Run OptimizeFn using gradient_ascent \n",
    "\n",
    "def loglikelihoodNew(w, X, y, alpha):\n",
    "    xbMatrix = np.dot(X, w) #TODO: (w,X) or (X,w) or (X.T, w)\n",
    "    prob = np.exp(y * xbMatrix)/(1 + np.exp(xbMatrix))\n",
    "    X = X.T\n",
    "    gradVal = np.dot(X, np.exp(xbMatrix)/(1 + np.exp(xbMatrix)))\n",
    "    gradValy = np.dot(X, y)\n",
    "    penalty = alpha/2 * np.sum(w**2)\n",
    "    gradPenalty = -alpha * w\n",
    "    return -np.sum(y * (xbMatrix) - np.log( 1 + np.exp(xbMatrix)))- (alpha / 2) * np.sum(w**2), -((gradValy - gradVal) - gradPenalty)\n",
    "\n",
    "def optimizeFnAsc( init_step, iterations, alpha, w, data, highlight):\n",
    "    g = lambda xy0: loglikelihoodNew(xy0, data, highlight, alpha)\n",
    "    f_val, update_asc_w = gradient_ascent( g, w, init_step, iterations )\n",
    "    return f_val, update_asc_w\n",
    "\n",
    "f_val_asc_new, update_asc_w_new =optimizeFnAsc( init_step = 1e-7, iterations=400, alpha=100, w = weights, data=dataNew, highlight=highlights)\n",
    "\n",
    "print(\"update_asc_w_new: \", update_asc_w_new)\n",
    "print(\"f_val_asc_new: \", f_val_asc_new)\n",
    "\n",
    "## 3b. Run OptimizaFn using gradient_descent\n",
    "def optimizeFnDes( init_step, iterations, alpha, w, data, highlight):\n",
    "    g = lambda xy0: loglikelihoodNew(xy0, data, highlight, alpha)\n",
    "    f_val, update_des_w = gradient_descent( g, w, init_step, iterations )\n",
    "    return f_val, update_des_w\n",
    "\n",
    "f_val_des_new, update_des_w_new =optimizeFnDes( init_step = 1e-7, iterations=200, alpha=300, w = weights, data=dataNew, highlight=highlights)\n",
    "\n",
    "print(\"update_des_w_new: \",update_des_w_new)\n",
    "print(\"f_val_des_new: \", f_val_des_new)\n",
    "\n",
    "## 4a. Run prediction on untrained weights\n",
    "untrained_predictions_new = prediction(weights, dataNew)\n",
    "\n",
    "print(\"prediction_new shape: \", untrained_predictions_new.shape)\n",
    "print(\"untrain_new: \", untrained_predictions_new)\n",
    "\n",
    "## 4b. Run prediction on trained weights\n",
    "trained_asc_predictions_new = prediction(update_asc_w_new, dataNew)\n",
    "\n",
    "print(trained_asc_predictions_new.shape)\n",
    "print(len(trained_asc_predictions_new))\n",
    "\n",
    "print(\"asc_new: \", trained_asc_predictions_new)\n",
    "\n",
    "## TODO: do it on updated weights using gradient descent\n",
    "trained_des_predictions_new = prediction(update_des_w_new, dataNew)\n",
    "\n",
    "print(trained_des_predictions_new.shape)\n",
    "print(len(trained_des_predictions_new))\n",
    "\n",
    "print(\"des_new: \", trained_des_predictions_new)\n",
    "\n",
    "## 5. Compare it against highlights\n",
    "def getAccuracy(highlights, predictions):\n",
    "    loss = 0\n",
    "    for k in range(len(predictions)):\n",
    "        if highlights[k] != predictions[k]:\n",
    "            loss += 1\n",
    "    return loss/len(highlights)\n",
    "\n",
    "for k in range(len(trained_des_predictions)):\n",
    "    if highlights[k] != trained_des_predictions[k]:\n",
    "        lossDes += 1\n",
    "\n",
    "lossUn_new = getAccuracy(highlights, untrained_predictions_new)\n",
    "lossAsc_new = getAccuracy(highlights, trained_asc_predictions_new)\n",
    "lossDes_new = getAccuracy(highlights, trained_des_predictions_new)\n",
    "print(\"accuracy for untrained_new data: \", lossUn_new)\n",
    "print(\"accuracy for gradient asc_new trained data: \", lossAsc_new)\n",
    "print(\"accuracy for gradient des_new trained data: \", lossDes_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
